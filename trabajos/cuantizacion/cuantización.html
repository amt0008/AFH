<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Manual Técnico: Cuantización de LLM con AutoAWQ</title>
    <style>
        /* Estilos base de cuantización.html */
        :root {
            --primary-color: #2563eb;
            --secondary-color: #1e40af;
            --bg-color: #f8fafc;
            --card-bg: #ffffff;
            --text-color: #1e293b;
            --border-radius: 12px;
        }

        html {
            scroll-behavior: smooth;
        }

        body {
            font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            background-color: var(--bg-color);
            max-width: 950px;
            margin: 0 auto;
            padding: 20px;
        }

        /* Header y Título */
        header {
            text-align: center;
            margin-bottom: 40px;
            padding: 40px 0;
            background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);
            border-radius: var(--border-radius);
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }

        h1 { color: var(--secondary-color); margin: 0; font-size: 2.5rem; }
        p.subtitle { color: #64748b; font-size: 1.1rem; margin-top: 10px; }

        /* Estilos del Índice (TOC) recuperados de cuantización.html */
        .toc-container {
            background-color: #ffffff;
            border: 1px solid #e2e8f0;
            border-radius: var(--border-radius);
            padding: 20px;
            margin-bottom: 40px;
        }

        .toc-title { font-weight: bold; margin-bottom: 15px; font-size: 1.2rem; color: var(--primary-color); }
        
        .toc-list {
            list-style: none;
            padding: 0;
            margin: 0;
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
        }

        .toc-list li a {
            text-decoration: none;
            color: #475569;
            background-color: #f1f5f9;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 0.9rem;
            transition: all 0.2s;
            display: block;
        }

        .toc-list li a:hover {
            background-color: var(--primary-color);
            color: white;
            transform: translateY(-2px);
        }

        /* Contenido Principal */
        h2 { 
            border-left: 5px solid var(--primary-color); 
            padding-left: 15px; 
            margin-top: 0;
            margin-bottom: 20px;
            color: var(--secondary-color);
            scroll-margin-top: 20px; /* Margen para el scroll del ancla */
        }
        
        h3 {
            color: var(--primary-color);
            margin-top: 25px;
            font-size: 1.1rem;
        }

        .step-card {
            background: var(--card-bg);
            border-radius: var(--border-radius);
            padding: 30px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
            border: 1px solid #f1f5f9;
        }

        .image-container {
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            overflow: hidden;
            background: #000;
            margin: 25px 0;
            text-align: center;
        }

        img {
            width: 100%;
            height: auto;
            display: block;
            transition: transform 0.3s ease;
            cursor: zoom-in;
        }

        img:hover { transform: scale(1.02); }

        .caption {
            font-size: 0.9em;
            color: #64748b;
            text-align: center;
            padding: 10px;
            background: #fff;
            display: block;
            border-top: 1px solid #e2e8f0;
        }

        code {
            background-color: #f1f5f9;
            padding: 2px 6px;
            border-radius: 4px;
            font-family: 'Courier New', monospace;
            color: #d63384;
            font-weight: bold;
        }

        /* Estilos para las Specs */
        .specs {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin-bottom: 30px;
        }

        .spec-card {
            background: var(--card-bg);
            padding: 20px;
            border-radius: var(--border-radius);
            text-align: center;
            border: 1px solid #e2e8f0;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        
        .spec-value {
            font-size: 1.2em;
            font-weight: bold;
            color: var(--primary-color);
            display: block;
            margin-bottom: 5px;
        }

        ul, ol { margin-left: 20px; color: #475569; }
        li { margin-bottom: 8px; }

        footer {
            text-align: center;
            margin-top: 80px;
            padding: 20px;
            border-top: 1px solid #e2e8f0;
            color: #94a3b8;
        }
    </style>
</head>
<body>

<header>
    <h1>Manual Técnico: Cuantización de LLM Local (AutoAWQ)</h1>
    <p class="subtitle">Documentación del pipeline de conversión de Mistral-7B a formato optimizado INT4</p>
</header>

<nav class="toc-container">
    <div class="toc-title">Tabla de Contenidos</div>
    <ul class="toc-list">
        <li><a href="#env-setup">1. Configuración del Entorno</a></li>
        <li><a href="#dependencies">2. Gestión de Dependencias</a></li>
        <li><a href="#code-logic">3. Lógica de Cuantización</a></li>
        <li><a href="#execution">4. Ejecución del Proceso</a></li>
        <li><a href="#verification">5. Verificación de Output</a></li>
        <li><a href="#inference">6. Pruebas de Inferencia</a></li>
    </ul>
</nav>

<div class="specs">
    <div class="spec-card">
        <span class="spec-value">NVIDIA RTX 3060</span>
        Hardware (12GB VRAM)
    </div>
    <div class="spec-card">
        <span class="spec-value">Mistral-7B-v0.3</span>
        Modelo Base
    </div>
    <div class="spec-card">
        <span class="spec-value">~15 GB ➔ 3.8 GB</span>
        Reducción de Peso
    </div>
    <div class="spec-card">
        <span class="spec-value">AutoAWQ</span>
        Algoritmo
    </div>
</div>

<section id="env-setup" class="step-card">
    <h2>1. Configuración del Entorno de Desarrollo</h2>
    <p>Para asegurar la integridad del sistema operativo y evitar la contaminación de librerías globales, es imperativo aislar el espacio de trabajo mediante un <strong>Entorno Virtual (venv)</strong>. Esta práctica estándar encapsula las bibliotecas de IA (PyTorch, AutoAWQ) en un directorio autocontenido.</p>
    
    <p>El procedimiento, visible en la terminal adjunta, implica la invocación de <code>python -m venv venv</code> y la posterior activación del <em>script</em> correspondiente en la shell de PowerShell.</p>

    <div class="image-container">
        <img src="images/image9.png" alt="Creación del entorno virtual">
        <span class="caption">Fig 1. Inicialización y activación del entorno virtual aislado en Windows.</span>
    </div>
</section>

<section id="dependencies" class="step-card">
    <h2>2. Resolución de Dependencias y Versionado</h2>
    <p>La implementación de LLMs locales frecuentemente enfrenta problemas de incompatibilidad de versiones (<em>Dependency Hell</em>). En esta instancia, se requirió una estrategia de instalación secuencial para mitigar conflictos conocidos entre <strong>NumPy 2.0</strong>, <strong>PyTorch 2.4+</strong> y el núcleo de AutoAWQ.</p>

    <h3>Estrategia de Despliegue:</h3>
    <ol>
        <li>Actualización del gestor <code>pip</code> para asegurar la descarga óptima de binarios precompilados (<em>wheels</em>).</li>
        <li>Instalación forzada de <strong>PyTorch 2.3.1</strong> con compatibilidad específica para CUDA 12.1.</li>
        <li><em>Downgrade</em> controlado de la librería <strong>Transformers</strong> a la versión 4.41.2 para garantizar estabilidad con AWQ.</li>
    </ol>

    <div class="image-container">
        <img src="images/image10.png" alt="Script de instalación de dependencias">
        <span class="caption">Fig 2. Ejecución de comandos para la resolución manual de dependencias.</span>
    </div>

    <p>Tras solventar las colisiones de librerías, el entorno quedó operativo con las versiones correctas de <code>autoawq</code>, <code>torch</code> y <code>accelerate</code>, listas para el cómputo intensivo.</p>

    <div class="image-container">
        <img src="images/image8.png" alt="Instalación exitosa">
        <span class="caption">Fig 3. Confirmación de despliegue exitoso de AutoAWQ 0.2.6 y sus prerequisitos.</span>
    </div>
</section>

<section id="code-logic" class="step-card">
    <h2>3. Lógica del Script de Cuantización</h2>
    <p>La orquestación del proceso recae sobre el script <code>quantizar_minstral.py</code>. Este código implementa la clase <code>AutoAWQForCausalLM</code> para gestionar la transformación de los tensores.</p>
    
    <h3>Parámetros de Configuración:</h3>
    <ul>
        <li><strong>q_group_size: 128:</strong> Determina el tamaño del grupo de cuantización, optimizando el balance entre degradación de señal y velocidad de cómputo.</li>
        <li><strong>w_bit: 4:</strong> Establece la compresión de pesos desde FP16 a INT4.</li>
        <li><strong>version: GEMM:</strong> Selecciona kernels optimizados para operaciones matriciales generales, ideales para la arquitectura Ampere de NVIDIA.</li>
    </ul>

    <div class="image-container">
        <img src="images/image1.png" alt="Código fuente del script de cuantización">
        <span class="caption">Fig 4. Definición del script Python para la ingesta y cuantización de Mistral-7B.</span>
    </div>
</section>

<section id="execution" class="step-card">
    <h2>4. Ejecución del Pipeline: Descarga y Cómputo</h2>
    <p>Al iniciar el script, se establece conexión con el repositorio de Hugging Face. Tratándose de un modelo "Instruct" de 7B parámetros, la descarga inicial de los tensores en precisión media (FP16) requiere aproximadamente <strong>14.5 GB</strong> de almacenamiento.</p>

    <div class="image-container">
        <img src="images/image7.png" alt="Descarga del modelo original">
        <span class="caption">Fig 5. Ingesta de los <em>shards</em> (fragmentos) del modelo base (~15GB).</span>
    </div>

    <p>Posteriormente, AutoAWQ carga el modelo en la RAM y emplea la GPU RTX 3060 para el cálculo de la "importancia de activación" (<em>Activation Awareness</em>). Este algoritmo identifica qué pesos son críticos para mantener la coherencia del modelo, permitiendo una compresión agresiva con mínima pérdida de capacidad cognitiva.</p>

    <div class="image-container">
        <img src="images/image6.png" alt="Proceso de cuantización finalizado">
        <span class="caption">Fig 6. Finalización del cómputo de cuantización y serialización a disco.</span>
    </div>
</section>

<section id="verification" class="step-card">
    <h2>5. Validación del Output Generado</h2>
    <p>El proceso culmina con la generación de un directorio autocontenido. La optimización es evidente: el modelo resultante ocupa apenas <strong>3.8 GB</strong>, facilitando su carga total en la VRAM de 12GB y liberando recursos significativos para la ventana de contexto (memoria a corto plazo).</p>

    <div class="image-container">
        <img src="images/image4.png" alt="Estructura de carpetas del proyecto">
        <span class="caption">Fig 7. Jerarquía de directorios mostrando el entorno virtual y el artifact de salida.</span>
    </div>

    <p>En el directorio de destino, verificamos la existencia del archivo <code>model.safetensors</code> consolidado, junto con la configuración del tokenizador necesaria para la correcta interpretación del texto.</p>

    <div class="image-container">
        <img src="images/image3.png" alt="Contenido de la carpeta del modelo cuantizado">
        <span class="caption">Fig 8. Archivos finales del modelo cuantizado listos para producción.</span>
    </div>
</section>

<section id="inference" class="step-card">
    <h2>6. Pruebas de Inferencia y Rendimiento</h2>
    <p>Para la validación funcional, se implementó el script <code>probar_cuantizacion.py</code>, incorporando técnicas de aceleración:</p>
    <ul>
        <li><strong>Layer Fusion:</strong> Fusión de capas de atención para reducir la latencia de acceso a memoria VRAM.</li>
        <li><strong>Streaming:</strong> Uso de <code>TextStreamer</code> para la generación de tokens en tiempo real, mejorando la latencia percibida.</li>
    </ul>

    <div class="image-container">
        <img src="images/image2.png" alt="Código del script de prueba">
        <span class="caption">Fig 9. Código de inferencia con soporte para plantillas de chat y gestión de historial.</span>
    </div>

    <h3>Evaluación Final</h3>
    <p>Se sometió al modelo a una consulta histórica compleja ("Carlos II de la corona hispánica"). La respuesta generada demostró precisión factual, coherencia sintáctica y una velocidad de generación superior, confirmando que la cuantización a 4 bits preservó exitosamente la base de conocimiento del modelo original.</p>

    <div class="image-container">
        <img src="images/image5.png" alt="Chat final funcionando">
        <span class="caption">Fig 10. Prueba de concepto exitosa: respuesta fluida y coherente en español.</span>
    </div>
</section>

<footer>
    <p>Documentación generada automáticamente para registro de MLOps local.</p>
</footer>

</body>
</html>